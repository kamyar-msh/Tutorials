{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to apply a topic modeling algorithm called *__\"Latent Dirichlet Allocation\"__*, which can be used to find the topics in documents. In LDA, topics are represented as a set of keywords, and each document can belong to different topics with different probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LDA it's supposed that there are certain topics in the corpus, each of which consists of a set of keywords (each keyword with a certain probability), and each document in the corpus belongs to some of those topics with different probabilities (to be more precise, to all of them, but to some of them with a really negligible probability). The main point about LDA is that LDA supposes that a document is a mixture of topics, and assumes that a document has been created by picking words from the topics, according to words distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(word) & = \\sum_{k=1}^{K} P(topic) \\: P(word \\: \\mid \\: topic_k) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure above is done in an iterative manner:\n",
    "<ol>\n",
    "1) Assigning documents to topics with different probabilities<br>\n",
    "2) Assigning words to the topics with different probabilities<br>\n",
    "3) Checking each word in the document and assigning it to one of the topics, according to the topics and words distributions<br>\n",
    "</ol>\n",
    "\n",
    "\\begin{align}\n",
    "P(topic_k \\: \\mid \\: doc) \\: P(word \\: \\mid \\: topic_k)\n",
    "\\end{align}\n",
    "\n",
    "The first thing to consider is that the initialization will be done randomly. The procedure above will be repeated until convergence or reaching some stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture below shows the procedure above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"first.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$ = Cluster of the word\n",
    "\n",
    "$W$ = The word\n",
    "\n",
    "$\\alpha$ = The distribution of topics in the document\n",
    "\n",
    "$\\eta$ = The distribution of keywords in a topic\n",
    "\n",
    "$\\theta$ = Topics in the document\n",
    "\n",
    "$\\beta$ = Keywords in a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α__ and __η__ are still unknown. We assume that those parameters come from two different __*Dirichlet distributions*__. Dirichlet distribution helps us to assume that both topics and keywords distributions are sparse. It means that we have only few topics, and for each topic few keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z_{d,n} \\mid \\theta \\sim Multinomial(\\theta,d) $\n",
    "\n",
    "$W_{d,n} \\mid Z_{d,n},\\beta \\sim Multinomial(\\beta_{Z_{d,n}}) $\n",
    "\n",
    "$\\beta_k \\mid  \\eta \\sim Dirichlet(\\eta) $\n",
    "\n",
    "$\\theta_d \\mid  \\alpha \\sim Dirichlet(\\alpha) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above, the only thing we have is the set of words (observable variable), based on which we need to find topics, keywords and words clusters (latent variables). Note that if a word belongs to a cluster (topic), it doesn’t mean that the word is among that topic keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(\\theta_{1,...,D},\\beta_{1,...,K},Z_{1,...,D;1,...,N}) \\mid W_{1,...,D;1,...,N})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$ = Number of topics\n",
    "\n",
    "$N_d$ = Number of words\n",
    "\n",
    "$D$ = Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some methods to solve above posterior, like sampling methods, which are not in the scope of this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
