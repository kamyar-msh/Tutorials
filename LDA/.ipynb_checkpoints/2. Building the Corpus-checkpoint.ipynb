{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Creating the Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of NSF Award abstracts is used in this notebook to be analyzed by LDA. the dataset can be found at __UCI Machine Learning Dataset__ collection by __Michael J. Pazzani__.\n",
    "\n",
    "The dataset contains thousands of files, which of them has some information about an article. Only abstracts and tags of articles are interesting for us. By means of these two we can compare topics resulted by LDA to original tags for each abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# For algorithms\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Part1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Corpus as dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the directory is made up of nested folders, we need to check all files in all folders. At the end we will have a dictionary with keys representing the name of the file, and values containing both tags and abstracts as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n"
     ]
    }
   ],
   "source": [
    "corpus = {}\n",
    "#tag_list = ['DMS', 'INT', 'DUE', 'EAR', 'OCE', 'ESI', 'CHE', 'DEB', 'CMS', 'MCB']\n",
    "\n",
    "counter = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for filename in files:\n",
    "        if '.txt' in filename:\n",
    "            file = open(os.path.join(root, filename),'r')\n",
    "            content = ' '.join([ii.strip() for ii in file.readlines()])\n",
    "            content = ' '.join(content.split())\n",
    "            org = re.findall(r'NSF Org :(.*?)Latest', content)[0].strip()\n",
    "            #if org in tag_list:\n",
    "            corpus[filename] = []\n",
    "            abstract = re.findall(r'Abstract :(.*)', content)[0].strip()\n",
    "            #abstract = clean_text(abstract)\n",
    "            corpus[filename].append(org)\n",
    "            corpus[filename].append(abstract)\n",
    "            counter += 1\n",
    "            if counter % 1000 == 0:\n",
    "                print(counter)\n",
    "            #else:\n",
    "                #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Corpus as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corpus = pd.DataFrame.from_dict(corpus, orient='index', columns=['tag', 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce tha size of the dataset, the rows, whose tags are among most common tags, are selected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = [ii[0] for ii in Counter(df_corpus['tag']).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corpus = df_corpus.loc[df_corpus['tag'].isin(tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent redoing cleaning part, the data is saved as a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('whole_corpus.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
